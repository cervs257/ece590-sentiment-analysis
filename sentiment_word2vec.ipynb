{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "This notebook is used to learn the embeddings of words in a dataset using the word2vec model."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "from datasets import load_dataset\n",
            "import torch\n",
            "import torchtext\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from datasets import load_from_disk\n",
            "import numpy as np\n",
            "\n",
            "import pandas as pd\n",
            "from datasets import Dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "seed = 257\n",
            "\n",
            "np.random.seed(seed)\n",
            "torch.manual_seed(seed)\n",
            "torch.cuda.manual_seed(seed)\n",
            "torch.backends.cudnn.deterministic = True"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Prepare the data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# load the dataset\n",
            "train_data, test_data = load_dataset(\"yelp_polarity\", split=[\"train\", \"test\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# tokenize the dataset\n",
            "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
            "\n",
            "\n",
            "def tokenize(obs, tokenizer, max_length=512):\n",
            "    \"\"\"\n",
            "    Tokenize an observation\n",
            "    max_length: the maximum length of the tokenized sequence\n",
            "    \"\"\"\n",
            "    return {\"tokens\": tokenizer(obs[\"text\"])[:max_length]}\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# remove stopwords and punctuation\n",
            "stop_words = stopwords.words(\"english\")\n",
            "\n",
            "\n",
            "def remove_stopwords(obs):\n",
            "    \"\"\"\n",
            "    Removes stopwords from tokens for each obs in Dataset\n",
            "    \"\"\"\n",
            "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word not in stop_words]\n",
            "    return obs\n",
            "\n",
            "\n",
            "def remove_punctuation(obs):\n",
            "    \"\"\"\n",
            "    Removes punctuation from tokens for each obs in Dataset\n",
            "    \"\"\"\n",
            "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word.isalpha()]\n",
            "    return obs\n",
            "\n",
            "\n",
            "def tokenize_and_clean(obs):\n",
            "    \"\"\"\n",
            "    Tokenize, remove stopwords and punctuation from observation\n",
            "    \"\"\"\n",
            "    tokens = tokenizer(obs[\"text\"][:512])\n",
            "    tokens = [word for word in tokens if word not in stop_words]\n",
            "    tokens = [word for word in tokens if word.isalpha()]\n",
            "    return {\"tokens\": tokens}\n",
            "\n",
            "\n",
            "# train_data = train_data.map(remove_stopwords)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# tokenizer(train_data[0][\"text\"][:512])\n",
            "train_data = train_data.map(tokenize_and_clean)\n",
            "# test_data = test_data.map(tokenize_and_clean)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# validation data\n",
            "train_valid_data = train_data.train_test_split(test_size=0.25)\n",
            "train_data = train_valid_data[\"train\"]\n",
            "valid_data = train_valid_data[\"test\"]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# train_data.save_to_disk(\"/datasets/yelp_polarity_train\")\n",
            "train_data = load_from_disk(\"/datasets/yelp_polarity_train/\")\n",
            "\n",
            "# valid_data.save_to_disk(\"/datasets/yelp_polarity_valid\")\n",
            "# valid_data = load_from_disk(\"/datasets/yelp_polarity_valid/\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "11739"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# creating the vocabulary\n",
            "special_tokens = [\"<unk>\"]\n",
            "\n",
            "# setting a minimum frequency for the tokens ... 100 times in 420,000 sentences is not a lot\n",
            "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
            "    train_data[\"tokens\"], specials=special_tokens, min_freq=75\n",
            ")\n",
            "vocab.set_default_index(vocab[\"<unk>\"])\n",
            "len(vocab)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "I'm going to create a new training dataset that contains word paris, such that, the first word is the input and the second word is the output (to be treated as a 'label'). \n",
            "\n",
            "Each observation will be input to a hidden layer comprised of 10 neurons. The weights of this hidden layer will be the word embeddings we want to learn."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_word_pairs(sentence, window_size=3):\n",
            "    \"\"\"\n",
            "    Generate word pairs from a sentence\n",
            "    \"\"\"\n",
            "    for i in range(len(sentence)):\n",
            "        for j in range(1, window_size + 1):\n",
            "            if i + j < len(sentence):\n",
            "                yield (sentence[i], sentence[i + j])\n",
            "            if i - j >= 0:\n",
            "                yield (sentence[i], sentence[i - j])\n",
            "\n",
            "def extract_pairs(dataset):\n",
            "    \"\"\"\n",
            "    Extract word pairs from dataset\n",
            "    \"\"\"\n",
            "    pairs = []\n",
            "    for i, obs in enumerate(dataset):\n",
            "        pairs.extend(get_word_pairs(obs[\"tokens\"]))\n",
            "    return pairs\n",
            "    "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# convert the new training data to a dataset from a DataFrame\n",
            "new_train = extract_pairs(train_data)\n",
            "\n",
            "new_train = Dataset.from_pandas(pd.DataFrame(new_train, columns=[\"x\", \"y\"]))\n",
            "\n",
            "# convert the new validation data to a dataset from a DataFrame\n",
            "# new_valid = extract_pairs(valid_data)\n",
            "# new_valid = Dataset.from_pandas(pd.DataFrame(new_valid, columns=[\"x\", \"y\"]))\n",
            "\n",
            "# # convert the new test data to a dataset from a DataFrame\n",
            "# new_test = extract_pairs(test_data)\n",
            "# new_test = Dataset.from_pandas(pd.DataFrame(new_test, columns=[\"x\", \"y\"]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "new_train.save_to_disk(\"/datasets/yelp_polarity_train_pairs\")\n",
            "# new_train = load_from_disk(\"/datasets/yelp_polarity_train_pairs\")\n",
            "\n",
            "# new_valid.save_to_disk(\"/datasets/yelp_polarity_valid_pairs\")\n",
            "# new_valid = load_from_disk(\"/datasets/yelp_polarity_valid_pairs\")\n",
            "\n",
            "# new_test.save_to_disk(\"/datasets/yelp_polarity_test_pairs\")\n",
            "# new_test = load_from_disk(\"/datasets/yelp_polarity_test_pairs\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(vocab.lookup_indices(\"dr\"))\n",
            "new_train[:3]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we need to numericalize our datasets. We'll use the vocabulary to convert each word to its corresponding index."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def numericalize_tokens(obs, vocab):\n",
            "    \"\"\"\n",
            "    Numericalize the tokens in the observation\n",
            "    \"\"\"\n",
            "    return {\n",
            "        \"x_ids\": vocab.lookup_indices(obs[\"x\"]),\n",
            "        \"y_ids\": vocab.lookup_indices(obs[\"y\"]),\n",
            "    }"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# numericalize the tokens\n",
            "new_train = new_train.map(numericalize_tokens, fn_kwargs={\"vocab\": vocab})\n",
            "# new_valid = new_valid.map(numericalize_tokens, fn_kwargs={\"vocab\": vocab})\n",
            "# new_test = new_test.map(numericalize_tokens, fn_kwargs={\"vocab\":vocab})\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we convert our datasets to PyTorch tensors."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "# new_train.save_to_disk(\"/datasets/yelp_polarity_train_pairs\")\n",
            "new_train = load_from_disk(\"/datasets/yelp_polarity_train_pairs\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see here."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "base",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
