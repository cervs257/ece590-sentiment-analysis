{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "This notebook is used to learn the embeddings of words in a dataset using the word2vec model."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "from datasets import load_dataset\n",
            "import torch\n",
            "import torchtext\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from datasets import load_from_disk\n",
            "import numpy as np\n",
            "\n",
            "import pandas as pd\n",
            "from datasets import Dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "seed = 257\n",
            "\n",
            "np.random.seed(seed)\n",
            "torch.manual_seed(seed)\n",
            "torch.cuda.manual_seed(seed)\n",
            "torch.backends.cudnn.deterministic = True"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Prepare the data\n",
            "\n",
            "We begin by tokenizing and cleaning the data. This process consists of removing punctuation, numbers, and stop words."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# load the dataset\n",
            "train_data, test_data = load_dataset(\"yelp_polarity\", split=[\"train\", \"test\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# tokenize the dataset\n",
            "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
            "\n",
            "\n",
            "def tokenize(obs, tokenizer, max_length=512):\n",
            "    \"\"\"\n",
            "    Tokenize an observation\n",
            "    max_length: the maximum length of the tokenized sequence\n",
            "    \"\"\"\n",
            "    return {\"tokens\": tokenizer(obs[\"text\"])[:max_length]}\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# remove stopwords and punctuation\n",
            "stop_words = stopwords.words(\"english\")\n",
            "\n",
            "\n",
            "def remove_stopwords(obs):\n",
            "    \"\"\"\n",
            "    Removes stopwords from tokens for each obs in Dataset\n",
            "    \"\"\"\n",
            "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word not in stop_words]\n",
            "    return obs\n",
            "\n",
            "\n",
            "def remove_punctuation(obs):\n",
            "    \"\"\"\n",
            "    Removes punctuation from tokens for each obs in Dataset\n",
            "    \"\"\"\n",
            "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word.isalpha()]\n",
            "    return obs\n",
            "\n",
            "\n",
            "def tokenize_and_clean(obs):\n",
            "    \"\"\"\n",
            "    Tokenize, remove stopwords and punctuation from observation\n",
            "    \"\"\"\n",
            "    tokens = tokenizer(obs[\"text\"][:512])\n",
            "    tokens = [word for word in tokens if word not in stop_words]\n",
            "    tokens = [word for word in tokens if word.isalpha()]\n",
            "    return {\"tokens\": tokens}\n",
            "\n",
            "\n",
            "# train_data = train_data.map(remove_stopwords)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# tokenizer(train_data[0][\"text\"][:512])\n",
            "train_data = train_data.map(tokenize_and_clean)\n",
            "# test_data = test_data.map(tokenize_and_clean)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# train_data.save_to_disk(\"/datasets/yelp_polarity_train\")\n",
            "# train_data = load_from_disk(\"/datasets/yelp_polarity_train/\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that our data has been tokenized and cleaned, we can create a validation set."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [],
         "source": [
            "# validation data\n",
            "train_valid_data = train_data.train_test_split(test_size=0.25)\n",
            "train_data = train_valid_data[\"train\"]\n",
            "valid_data = train_valid_data[\"test\"]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "From the training data, we now proceed to create a vocabulary comprised of the training data's unique words (if they appear more than 75 times)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "11739"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# creating the vocabulary\n",
            "special_tokens = [\"<unk>\"]\n",
            "\n",
            "# setting a minimum frequency for the tokens ... 75 times in 420,000 sentences is not a lot\n",
            "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
            "    train_data[\"tokens\"], specials=special_tokens, min_freq=75\n",
            ")\n",
            "vocab.set_default_index(vocab[\"<unk>\"])\n",
            "len(vocab)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that we have the vocabulary, we can numerically encode the words in the training data."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "application/vnd.jupyter.widget-view+json": {
                     "model_id": "51f3064f2bc941a295e47184289a0383",
                     "version_major": 2,
                     "version_minor": 0
                  },
                  "text/plain": [
                     "Map:   0%|          | 0/420000 [00:00<?, ? examples/s]"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "def numericalize_example(obs, vocab):\n",
            "    ids = vocab.lookup_indices(obs[\"tokens\"])\n",
            "    return {\"ids\": ids}\n",
            "\n",
            "\n",
            "train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that we have numericalized the data, we can create word pairs for the skip-gram model. Since we're already iterating over the entire dataset, we'll convert the data from indexes to tensors."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [],
         "source": [
            "# find a way to incorporate transformation to tensors in this process\n",
            "\n",
            "def get_word_pairs(sentence, window_size=3):\n",
            "    \"\"\"\n",
            "    Generate word pairs from a sentence\n",
            "    \"\"\"\n",
            "    for i, token in enumerate(sentence):\n",
            "        for j in range(1, window_size + 1):\n",
            "            if i + j < len(sentence):\n",
            "                yield (torch.tensor(sentence[i]), torch.tensor(sentence[i + j]))\n",
            "            if i - j >= 0:\n",
            "                yield (torch.tensor(sentence[i]), torch.tensor(sentence[i - j]))\n",
            "\n",
            "\n",
            "def extract_pairs(dataset):\n",
            "    \"\"\"\n",
            "    Extract word pairs from dataset\n",
            "    \"\"\"\n",
            "    pairs = []\n",
            "    for i, obs in enumerate(dataset):\n",
            "        pairs.extend(get_word_pairs(obs[\"tokens\"]))\n",
            "    return pairs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [],
         "source": [
            "# convert the new training data to a dataset from a DataFrame\n",
            "new_train = extract_pairs(train_data)\n",
            "\n",
            "new_train = Dataset.from_pandas(pd.DataFrame(new_train, columns=[\"x\", \"y\"]))\n",
            "\n",
            "# convert the new validation data to a dataset from a DataFrame\n",
            "# new_valid = extract_pairs(valid_data)\n",
            "# new_valid = Dataset.from_pandas(pd.DataFrame(new_valid, columns=[\"x\", \"y\"]))\n",
            "\n",
            "# # convert the new test data to a dataset from a DataFrame\n",
            "# new_test = extract_pairs(test_data)\n",
            "# new_test = Dataset.from_pandas(pd.DataFrame(new_test, columns=[\"x\", \"y\"]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "new_train.save_to_disk(\"/datasets/yelp_polarity_train_pairs\")\n",
            "# new_train = load_from_disk(\"/datasets/yelp_polarity_train_pairs\")\n",
            "\n",
            "# new_valid.save_to_disk(\"/datasets/yelp_polarity_valid_pairs\")\n",
            "# new_valid = load_from_disk(\"/datasets/yelp_polarity_valid_pairs\")\n",
            "\n",
            "# new_test.save_to_disk(\"/datasets/yelp_polarity_test_pairs\")\n",
            "# new_test = load_from_disk(\"/datasets/yelp_polarity_test_pairs\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we convert our datasets to PyTorch tensors."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "# new_train.save_to_disk(\"/datasets/yelp_polarity_train_pairs\")\n",
            "new_train = load_from_disk(\"/datasets/yelp_polarity_train_pairs\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see here."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "base",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
