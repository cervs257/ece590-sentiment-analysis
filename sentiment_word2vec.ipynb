{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to learn the embeddings of words in a dataset using the word2vec model."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": null,
>>>>>>> 60e8729 (numericalize)
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torchtext\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 97,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 257\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 60e8729 (numericalize)
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train_data, test_data = load_dataset(\"yelp_polarity\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 101,
=======
   "execution_count": null,
>>>>>>> 60e8729 (numericalize)
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def tokenize(obs, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize an observation\n",
    "    max_length: the maximum length of the tokenized sequence\n",
    "    \"\"\"\n",
    "    return {\"tokens\": tokenizer(obs[\"text\"])[:max_length]}\n",
    "\n",
    "\n",
    "# train_data = train_data.map(\n",
    "#     tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": 512}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
=======
   "execution_count": null,
>>>>>>> 60e8729 (numericalize)
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and punctuation\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def remove_stopwords(obs):\n",
    "    \"\"\"\n",
    "    Removes stopwords from tokens for each obs in Dataset\n",
    "    \"\"\"\n",
    "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word not in stop_words]\n",
    "    return obs\n",
    "\n",
    "\n",
    "def remove_punctuation(obs):\n",
    "    \"\"\"\n",
    "    Removes punctuation from tokens for each obs in Dataset\n",
    "    \"\"\"\n",
    "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word.isalpha()]\n",
    "    return obs\n",
    "\n",
    "\n",
    "def tokenize_and_clean(obs):\n",
    "    \"\"\"\n",
    "    Tokenize, remove stopwords and punctuation from observation\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(obs[\"text\"][:512])\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [word for word in obs[\"tokens\"] if word.isalpha()]\n",
    "    return {\"tokens\": tokens}\n",
    "\n",
    "\n",
    "# train_data = train_data.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83cebbe7a69b44a6ab9cdc92729b9b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/560000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 60e8729 (numericalize)
   "source": [
    "# tokenizer(train_data[0][\"text\"][:512])\n",
    "train_data = train_data.map(tokenize_and_clean)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a377d8742f1e438ebdbf056399625c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/560000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 60e8729 (numericalize)
   "source": [
    "# train_data.save_to_disk(\"/datasets/yelp_polarity_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_from_disk(\"/datasets/yelp_polarity_train/\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": null,
>>>>>>> 60e8729 (numericalize)
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "train_valid_data = train_data.train_test_split(test_size=0.25)\n",
    "train_data = train_valid_data[\"train\"]\n",
    "valid_data = train_valid_data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 84,
=======
   "execution_count": null,
>>>>>>> 60e8729 (numericalize)
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary\n",
    "special_tokens = [\"<unk>\"]\n",
    "\n",
    "# setting a minimum frequency for the tokens ... 100 times in 420,000 sentences is not a lot\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"tokens\"], specials=special_tokens, min_freq=100\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11863"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate one-hot encoding for the tokens\n",
    "def one_hot_encoding(obs, vocab):\n",
    "    \"\"\"\n",
    "    Generate one-hot encoding for the tokens\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for token in obs[\"tokens\"]:\n",
    "        embedding = np.zeros(len(vocab))\n",
    "        embedding[vocab[token]] = 1\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    return {\"embeddings\": embeddings}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to create a new training dataset that contains word paris, such that, the first word is the input and the second word is the output (to be treated as a 'label'). \n",
    "\n",
    "Each observation will be input to a hidden layer comprised of 10 neurons. The weights of this hidden layer will be the word embeddings we want to learn."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": null,
>>>>>>> 60e8729 (numericalize)
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new training data with one-hot encoding\n",
    "\n",
    "def get_word_pairs(sentence, window_size=3):\n",
    "    \"\"\"\n",
    "    Generate word pairs from a sentence\n",
    "    \"\"\"\n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(1, window_size + 1):\n",
    "            if i + j < len(sentence):\n",
    "                yield (sentence[i], sentence[i + j])\n",
    "            if i - j >= 0:\n",
    "                yield (sentence[i], sentence[i - j])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we extract each word pair from each example in the training data\n",
    "new_train = []\n",
    "for obs in train_data:\n",
    "    word_pairs = get_word_pairs(obs[\"tokens\"])\n",
    "    for pair in word_pairs:\n",
    "        new_train.append(pair)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the new training data to a dataset from a DataFrame\n",
    "new_train = extract_pairs(train_data)\n",
    "\n",
    "new_train = Dataset.from_pandas(pd.DataFrame(new_train, columns=[\"x\", \"y\"]))\n",
    "\n",
    "# convert the new validation data to a dataset from a DataFrame\n",
    "# new_valid = extract_pairs(valid_data)\n",
    "# new_valid = Dataset.from_pandas(pd.DataFrame(new_valid, columns=[\"x\", \"y\"]))\n",
    "\n",
    "# # convert the new test data to a dataset from a DataFrame\n",
    "# new_test = extract_pairs(test_data)\n",
    "# new_test = Dataset.from_pandas(pd.DataFrame(new_test, columns=[\"x\", \"y\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.save_to_disk(\"/datasets/yelp_polarity_train_pairs\")\n",
    "# new_train = load_from_disk(\"/datasets/yelp_polarity_train_pairs\")\n",
    "\n",
    "# new_valid.save_to_disk(\"/datasets/yelp_polarity_valid_pairs\")\n",
    "# new_valid = load_from_disk(\"/datasets/yelp_polarity_valid_pairs\")\n",
    "\n",
    "# new_test.save_to_disk(\"/datasets/yelp_polarity_test_pairs\")\n",
    "# new_test = load_from_disk(\"/datasets/yelp_polarity_test_pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab.lookup_indices(\"dr\"))\n",
    "new_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to numericalize our datasets. We'll use the vocabulary to convert each word to its corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_tokens(obs, vocab):\n",
    "    \"\"\"\n",
    "    Numericalize the tokens in the observation\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"x_ids\": vocab.lookup_indices(obs[\"x\"]),\n",
    "        \"y_ids\": vocab.lookup_indices(obs[\"y\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalize the tokens\n",
    "new_train = new_train.map(numericalize_tokens, fn_kwargs={\"vocab\": vocab})\n",
    "# new_valid = new_valid.map(numericalize_tokens, fn_kwargs={\"vocab\": vocab})\n",
    "# new_test = new_test.map(numericalize_tokens, fn_kwargs={\"vocab\":vocab})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert our datasets to PyTorch tensors."
>>>>>>> 60e8729 (numericalize)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "# convert the new training data to a pandas dataframe\n",
    "df_train = pd.DataFrame(new_train, columns=[\"x\", \"y\"])\n",
    "\n",
    "# convert the new training data to a dataset\n",
    "new_train = Dataset.from_pandas(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a27f6bd8b54f64a4250eb903e8398f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/6 shards):   0%|          | 0/150240126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
>>>>>>> 60e8729 (numericalize)
   "source": [
    "# new_train.save_to_disk(\"/datasets/yelp_polarity_train_pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into X_train and y_train\n",
    "X_train = new_train[\"x\"]\n",
    "y_train = new_train[\"y\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
